{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wfdb\n",
    "import scipy as sp\n",
    "\n",
    "multiprocessing.set_start_method(\"fork\")\n",
    "\n",
    "# Load data\n",
    "ecg = wfdb.rdsamp(\"../data/ECG/ath_001\")\n",
    "ecg_resampled = sp.signal.resample(ecg[0], 1000, axis=0)\n",
    "x = np.expand_dims(np.transpose(ecg_resampled), axis=0).astype(np.float32).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The given NumPy array is not writable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarks\n",
    "pt_us = 114  # 0.11 ms\n",
    "crypten_us = 332 * 1e3  # 332 ms\n",
    "concrete_us = 33.3 * 1e6  # 33 sek\n",
    "tenseal_us = (3*60 + 56) * 1e6  # 4 min\n",
    "\n",
    "print(f\"CrypTen  is {int(crypten_us/pt_us):,} times slower\")\n",
    "print(f\"Concrete is {int(concrete_us/pt_us):,} times slower\")\n",
    "print(f\"TenSEAL  is {int(tenseal_us/pt_us):,} times slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plaintext models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1M params\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(12000, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 71)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6M params\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, hidden=128, output=71):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # Input: (n, 12, 1000)\n",
    "        self.conv1 = nn.Conv1d(12, 36, kernel_size=1, bias=False)\n",
    "        \n",
    "        # Calculate the output length from formula\n",
    "        # After conv1: (n, 36, 1000)\n",
    "        self.fc1 = nn.Linear(int(36*1000), hidden)\n",
    "        self.fc2 = nn.Linear(hidden, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  \n",
    "        x = x * x\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = x * x\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "mlp = MLP()\n",
    "mlp_input = torch.randn(1, 12000)\n",
    "torch.save(mlp, \"mlp.pt\")\n",
    "torch.onnx.export(\n",
    "    mlp,\n",
    "    mlp_input,\n",
    "    \"mlp.onnx\",\n",
    "    export_params=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=14,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "torch.onnx.export(\n",
    "    mlp,\n",
    "    mlp_input,\n",
    "    \"../cryptflow/mlp.onnx\",\n",
    "    export_params=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=14,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    "    keep_initializers_as_inputs=False,\n",
    ")\n",
    "\n",
    "convnet = ConvNet()\n",
    "convnet_input = torch.randn(1, 12, 1000)\n",
    "torch.save(convnet, \"convnet.pt\")\n",
    "torch.onnx.export(\n",
    "    convnet,\n",
    "    convnet_input,\n",
    "    \"convnet.onnx\",\n",
    "    export_params=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=14,\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    "    keep_initializers_as_inputs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX plaintext inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_session = rt.InferenceSession(\"mlp.onnx\")\n",
    "out_pt = mlp_session.run([\"output\"], {\"input\": x})[0]\n",
    "out_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "mlp_session.run([\"output\"], {\"input\": x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concrete ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from concrete.ml.torch.compile import compile_onnx_model\n",
    "\n",
    "model = onnx.load(\"mlp.onnx\")\n",
    "input_set = np.random.normal(size=(1, 12000))\n",
    "\n",
    "cml_model = compile_onnx_model(\n",
    "    model,\n",
    "    input_set,\n",
    "    n_bits=6,\n",
    "    rounding_threshold_bits={\"n_bits\": 6, \"method\": \"approximate\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to encrypt input?\n",
    "out_cml = cml_model.forward(x, fhe=\"execute\")\n",
    "out_cml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "cml_model.forward(x, fhe=\"execute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TenSeal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tenseal as ts\n",
    "\n",
    "class EncMLP:\n",
    "    def __init__(self, model):\n",
    "        self.fc1_weight = model.state_dict()[\"fc1.weight\"].transpose(0, 1)\n",
    "        self.fc1_bias = model.state_dict()[\"fc1.bias\"]\n",
    "        self.fc2_weight = model.state_dict()[\"fc2.weight\"].transpose(0, 1)\n",
    "        self.fc2_bias = model.state_dict()[\"fc2.bias\"]\n",
    "        self.fc3_weight = model.state_dict()[\"fc3.weight\"].transpose(0, 1)\n",
    "        self.fc3_bias = model.state_dict()[\"fc3.bias\"]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.fc1_weight) + self.fc1_bias\n",
    "        x.square_()        \n",
    "        x = x.mm(self.fc2_weight) + self.fc2_bias \n",
    "        x.square_()        \n",
    "        x = x.mm(self.fc3_weight) + self.fc3_bias \n",
    "        return x\n",
    "\n",
    "\n",
    "# Encryption parameters\n",
    "# TODO: Figure out how to set these\n",
    "bits_scale = 26\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=32768,\n",
    "    coeff_mod_bit_sizes=[31, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
    ")\n",
    "context.global_scale = pow(2, bits_scale)\n",
    "context.generate_galois_keys()  # Required to do ciphertext rotations\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"mlp.pt\")\n",
    "ts_model = EncMLP(model)\n",
    "\n",
    "# Encrypt input\n",
    "x_ts_enc = ts.ckks_vector(context, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ts_enc = ts_model.forward(x_ts_enc)\n",
    "out_ts = out_ts_enc.decrypt()\n",
    "out_ts = np.asarray(out_ts).reshape(1, -1)\n",
    "out_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ts_model.forward(x_ts_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrypTen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import crypten.mpc as mpc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "crypten.init()\n",
    "# Disables OpenMP threads -- needed by @mpc.run_multiprocess which uses fork\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "\n",
    "CLIENT = 0\n",
    "SERVER = 1\n",
    "\n",
    "crypten.common.serial.register_safe_class(MLP)\n",
    "\n",
    "model = MLP()\n",
    "torch.save(model, \"mlp_local.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def run():\n",
    "    dummy_model = MLP()\n",
    "    dummy_input = torch.empty((1, 12000))\n",
    "    \n",
    "    # Encrypt model\n",
    "    model_data = torch.load(\"mlp_local.pt\", weights_only=False)\n",
    "    dummy_model.load_state_dict(model_data.state_dict())\n",
    "    private_model = crypten.nn.from_pytorch(dummy_model, dummy_input)\n",
    "    private_model.encrypt(src=SERVER)\n",
    "\n",
    "    # Encrypt data\n",
    "    data = torch.load(\"input.pth\")\n",
    "    data_enc = crypten.cryptensor(data, src=CLIENT)\n",
    "\n",
    "    # Encrypted inference\n",
    "    private_model.eval()\n",
    "    out_enc = private_model(data_enc)\n",
    "    out = out_enc.get_plain_text()\n",
    "    # crypten.print(f\"Output: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
